{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RazerRaymond/MachineLearningPS/blob/main/Evaluating_Gesture_Recognition_using_NNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya2QmifUEvWs"
      },
      "source": [
        "# Evaluating Gesture Recognition using NNs\n",
        "\n",
        "Data was based on WUSTL CSE 217A data in SP2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP6OttQTEvWx"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "The data needed for this assignemnt can be found [here](https://wustl.box.com/s/q8mnl1o2zq2bh0ca5zajtk3msnu03ou8). All of it was gathered in `Homework 10 (Part I)`: \n",
        "- training\n",
        "- validation\n",
        "- augmented\n",
        "- testing\n",
        "\n",
        "Here are the neural network models trained on `training`:\n",
        "- cse217_v1.h5 (still training; watch for announcement on Piazza)\n",
        "- cse217_v2.h5 (still training; watch for announcement on Piazza)\n",
        "\n",
        "Here are the neural network models trained on `augmented`:\n",
        "- cse217_v1_augmented.h5 (still training; watch for announcement on Piazza)\n",
        "- cse217_v2_augmented.h5 (still training; watch for announcement on Piazza)\n",
        "\n",
        "Note that to train these models we used the `validation` dataset to determine when to stop the training process. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UawEY4RgEvWx"
      },
      "source": [
        "## 2. Test Data Collection, Data Profiling, and Model Understanding\n",
        "\n",
        "In this section, we will get a feel for our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejETmCFgEvWy"
      },
      "outputs": [],
      "source": [
        "from os import makedirs, mkdir\n",
        "from os.path import exists\n",
        "\n",
        "base = 'utility/data'\n",
        "raw = f'{base}/raw'\n",
        "dirs = ['rock', 'paper', 'scissors']\n",
        "\n",
        "if not exists(raw):\n",
        "    makedirs(raw, exist_ok=True)\n",
        "\n",
        "for sign in dirs:\n",
        "    path = f'{raw}/{sign}'\n",
        "    \n",
        "    if not exists(path):\n",
        "        mkdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxlS39TfEvWz"
      },
      "source": [
        "Store the images you took of rocks (‚úä), papers (ü§ö), and scissors (‚úåÔ∏è) in the correct folders in `utility/data/raw`. Then, run the following cell to produced rescaled images, which will be stored in `utility/data/testing`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ5pEyr0EvWz",
        "outputId": "3482f8f6-0fcc-45eb-88f0-d6bc0e04f97d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ray/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/home/ray/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "utility/data/raw/scissors/4.jpg\n",
            "utility/data/raw/scissors/2.jpg\n",
            "utility/data/raw/scissors/5 (1).jpg\n",
            "utility/data/raw/scissors/3.jpg\n",
            "utility/data/raw/scissors/1.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "utility/data/raw/rock/4.jpg\n",
            "utility/data/raw/rock/2.jpg\n",
            "utility/data/raw/rock/5.jpg\n",
            "utility/data/raw/rock/3.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "utility/data/raw/rock/1.jpg\n",
            "utility/data/raw/paper/4.jpg\n",
            "utility/data/raw/paper/5 (2).jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "utility/data/raw/paper/2.jpg\n",
            "utility/data/raw/paper/3.jpg\n",
            "utility/data/raw/paper/1.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "from utility.util import load_image, resize_image, save_image\n",
        "\n",
        "\n",
        "testing = f'{base}/testing'\n",
        "\n",
        "for sign in dirs:\n",
        "    path = f'{testing}/{sign}'\n",
        "    \n",
        "    if not exists(path):\n",
        "        makedirs(path, exist_ok=True)\n",
        "\n",
        "for path, _, files in os.walk(raw):\n",
        "    sign = os.path.basename(path)\n",
        "\n",
        "    for file in files:\n",
        "        input_path = f'{path}/{file}'\n",
        "        output_path = f'{testing}/{sign}/{file}'\n",
        "        \n",
        "        # note! warnings about lossy conversion are ok\n",
        "        image = load_image(input_path)\n",
        "        image = resize_image(image, (500, 500))\n",
        "\n",
        "        save_image(output_path, image)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ANTbHpiwEvW0"
      },
      "source": [
        "### Check if the Dataset is balanced\n",
        "\n",
        "testing dataset: 5 rock (c0), 5 paper (c1), 5 scissors (c2)\n",
        "training dataset: 266 rock (c0), 266 paper (c1), 264 scissors (c2)\n",
        "validation dataset: 66 rock (c0), 66 paper (c1), 66 scissors (c2)\n",
        "augmented dataset: 1237 rock (c0), 1255 paper (c1), 1258 scissors (c2)\n",
        "\n",
        "The datasets are balanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghDNPyUbEvW1",
        "outputId": "29f0f06a-98b4-4c12-d1ed-bf905d073401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ray/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "v1_raw = load_model('/home/ray/Downloads/hwhw10/utility/model.v1.raw.h5', compile=False)\n",
        "v1_aug = load_model('/home/ray/Downloads/hwhw10/utility/model.v1.augmented.h5', compile=False)\n",
        "v2_raw = load_model('/home/ray/Downloads/hwhw10/utility/model.v2.raw.h5', compile=False)\n",
        "v2_aug = load_model('/home/ray/Downloads/hwhw10/utility/model.v2.augmented.h5', compile=False)\n",
        "\n",
        "#v1_raw.summary()\n",
        "#v1_aug.summary()\n",
        "#v2_raw.summary()\n",
        "#v2_aug.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRtTcjD0EvW1"
      },
      "source": [
        "## 3. Model Comparison: v1 vs v2\n",
        "\n",
        "By now we should know all of the ins and outs about our datasets and models. Let's evaluate and compare the models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djg7TafcEvW1"
      },
      "source": [
        "### None Augmented dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukknbh8TEvW2"
      },
      "source": [
        "In the following cell, we provide an example of how to load the testing. Note the dimensions of the dataset (especially the size of the images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3ToSJh0EvW2"
      },
      "outputs": [],
      "source": [
        "def accuracy(X, y, model):\n",
        "    acc = 0\n",
        "    label =  y\n",
        "    image = X\n",
        "    patchsize = model.input_shape[1]\n",
        "    for i in range(0,len(X)):\n",
        "        image_tran = skimage.transform.resize(image[i], (patchsize,patchsize))\n",
        "        outs = model.predict(np.array([image_tran]))\n",
        "        predicted = np.argmax(outs)\n",
        "        if predicted == np.argmax(label[i]):\n",
        "            acc+=1\n",
        "    \n",
        "    print(\"Number of pictures predicted correctly by model: %d\" % acc)\n",
        "    print(\"Number of picutres in the dataset: %d\" % len(X))\n",
        "\n",
        "    return acc/len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGqeBDziEvW2",
        "outputId": "d3f0fab2-3c35-4777-ef64-6021d3aec353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "utility/data/testing/scissors/4.jpg\n",
            "utility/data/testing/scissors/2.jpg\n",
            "utility/data/testing/scissors/5 (1).jpg\n",
            "utility/data/testing/scissors/3.jpg\n",
            "utility/data/testing/scissors/1.jpg\n",
            "utility/data/testing/rock/4.jpg\n",
            "utility/data/testing/rock/2.jpg\n",
            "utility/data/testing/rock/5.jpg\n",
            "utility/data/testing/rock/3.jpg\n",
            "utility/data/testing/rock/1.jpg\n",
            "utility/data/testing/paper/4.jpg\n",
            "utility/data/testing/paper/5 (2).jpg\n",
            "utility/data/testing/paper/2.jpg\n",
            "utility/data/testing/paper/3.jpg\n",
            "utility/data/testing/paper/1.jpg\n",
            "Number of pictures predicted correctly by model: 2\n",
            "Number of picutres in the dataset: 15\n",
            "Number of pictures predicted correctly by model: 5\n",
            "Number of picutres in the dataset: 15\n",
            "v1_raw on test: \n",
            "0.13333333333333333\n",
            "v2_raw on test: \n",
            "0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "from utility.util import load_dataset\n",
        "import skimage\n",
        "import numpy as np\n",
        "\n",
        "target_shape = (500, 500)\n",
        "X_test_example, y_test_example = load_dataset('utility/data/testing', target_shape)\n",
        "v1_t = accuracy(X_test_example, y_test_example, v1_raw) \n",
        "v2_t = accuracy(X_test_example, y_test_example, v2_raw) \n",
        "print(\"v1_raw on test: \")\n",
        "print(v1_t)\n",
        "print(\"v2_raw on test: \")\n",
        "print(v2_t)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_shape = (500, 500)\n",
        "X_train, y_train = load_dataset('/home/ray/Downloads/hwhw10/utility/data/training', target_shape)\n",
        "v1_train = accuracy(X_train, y_train, v1_raw) \n",
        "v2_train = accuracy(X_train, y_train, v2_raw) \n",
        "print(\"v1_raw on train: \")\n",
        "print(v1_train)\n",
        "print(\"v2_raw on train: \")\n",
        "print(v2_train)"
      ],
      "metadata": {
        "id": "_AeDCWU5Fbyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_shape = (500, 500)\n",
        "X_valid, y_valid = load_dataset('/home/ray/Downloads/hwhw10/utility/data/validation', target_shape)\n",
        "v1_v = accuracy(X_valid, y_valid, v1_raw) \n",
        "v2_v = accuracy(X_valid, y_valid, v2_raw) \n",
        "print(\"v1_raw on validation: \")\n",
        "print(v1_v)\n",
        "print(\"v2_raw on validation: \")\n",
        "print(v2_v)"
      ],
      "metadata": {
        "id": "n29Dwl6YFdVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "at30mUfCEvW2"
      },
      "source": [
        "\n",
        "              | training acc | validation acc | testing acc \n",
        "    cse217_v1 |     0.4643   |      0.4308    |    0.13\n",
        "    cse217_v2 |     0.5740   |      0.4718    |    0.33\n",
        "v1_raw on test: \n",
        "0.13333333333333333\n",
        "v2_raw on test: \n",
        "0.3333333333333333\n",
        "v1_raw on train: \n",
        "0.4642857142857143 \n",
        "v2_raw on train: \n",
        "0.5739795918367347\n",
        "v1_raw on validation: \n",
        "0.4307692307692308\n",
        "v2_raw on validation: \n",
        "0.4717948717948718\n",
        "\n",
        "Based on the presented data, the v2 model performs better than the v1 model on the all non-augmented data.\n",
        "It behaved in this way because v2 model involving in more parameters and trained more time, that it \"learned\" more from data.\n",
        "\n",
        "I dont think we are satisfied with the current accuracy. One of the ways we can improve is to trained the model on more datasets, so it generates better to the new data."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "tz9GwitPEvW3"
      },
      "source": [
        "### Prediction Result Analysis\n",
        "Of all the images of rocks, 5 predictions of rock are all incorrect,recognizing them as paper.\n",
        "Of all the images of sissors, 2 predictions are incorrect,recognizing them as paper, and 3 of them are correct.\n",
        "Of all the images of papers, 2 predictions are incorrect,recognizing them as sissors and rock, and 3 of them are correct.\n",
        "\n",
        "Based on my observation, rock and paper are mixed most frequently, with sissors and papers followed behind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk2uFNRXEvW3"
      },
      "outputs": [],
      "source": [
        "def create_user_testdata(path2folder, foldername):\n",
        "    dataset_directory = pathlib.Path(path2folder)\n",
        "\n",
        "    # Now check the data\n",
        "    ddir=dataset_directory/foldername\n",
        "    cdirs={}\n",
        "    cdirs.update({ddir/\"rock\":0,\n",
        "                  ddir/\"paper\":1,\n",
        "                  ddir/\"scissors\":2})\n",
        "\n",
        "    names = [\"rock\", \"paper\", \"scissors\"]\n",
        "\n",
        "    for cdir,cdir_class in cdirs.items():\n",
        "        assert cdir.exists()==1, str(cdir)+' does not exist'\n",
        "        print(\"Found directory {} containing class {}\".format(cdir,names[cdir_class]))\n",
        "\n",
        "    imagesize = 500\n",
        "    dataset1=[]\n",
        "    for cdir,cn in reversed(list(cdirs.items())):\n",
        "\n",
        "        for f in tqdm(list(cdir.glob(\"*\"))):\n",
        "            try:\n",
        "                im=skimage.io.imread(f)\n",
        "                h,w=im.shape[0:2] # height, width\n",
        "                sz=min(h,w)\n",
        "                im=im[(h//2-sz//2):(h//2+sz//2),(w//2-sz//2):(w//2+sz//2),:] # defines the central square\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\")\n",
        "                    im=skimage.img_as_ubyte(skimage.transform.resize(im,(imagesize,imagesize))) # resize it to 500x500, whatever the original resolution\n",
        "            except:\n",
        "                warnings.warn(\"ignoring \"+str(f))\n",
        "                continue\n",
        "\n",
        "            dataset1.append({\n",
        "                \"file\": f,\n",
        "                \"label\": cn,\n",
        "                \"image\": im\n",
        "            })\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "    dataset1 = pd.DataFrame(dataset1)\n",
        "    dataset1[\"dn\"] = dataset1[\"file\"].apply(lambda x: x.parent.parts[-2])\n",
        "    return dataset1\n",
        "\n",
        "# Show results by processing a single validataion or testing image\n",
        "names = [\"rock\", \"paper\", \"scissors\"]\n",
        "\n",
        "%matplotlib inline\n",
        "def resultsShow(i, data, model):\n",
        "    guide = { 0:\"rock\",1:\"paper\",2:\"scissor\"}\n",
        "    d = data.iloc[i]\n",
        "    im = d[\"image\"]\n",
        "    l = d[\"label\"]\n",
        "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
        "    \n",
        "    imt = imr = skimage.transform.resize(im, (model.input_shape[1],model.input_shape[1]))\n",
        "    axs[0].imshow(im)\n",
        "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
        "    \n",
        "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
        "    axs[1].set_title(\"Network input\")\n",
        "    \n",
        "    outs = model.predict(np.array([imt]))\n",
        "    predicted = np.argmax(outs)\n",
        "    print(outs)\n",
        "    print(\"predicted label, %s\" % guide.get(predicted))\n",
        "    print(\"actual label, %s\"% guide.get(l))\n",
        "\n",
        "    axs[2].bar(np.array(range(len(names)))-0.5, outs[0,:], 1, color=\"gray\")\n",
        "    axs[2].set_ylim([0,1])\n",
        "    axs[2].set_xticks(range(len(names)))\n",
        "    axs[2].set_xticklabels(names)\n",
        "    axs[2].set_ylabel(\"probability\")\n",
        "    axs[2].set_xlabel(\"class\")\n",
        "    axs[2].set_title(\"Network output\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    #fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqoXTERyEvW3",
        "outputId": "aa3d6de6-491d-4ee6-f89f-e9006febc04b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:00<00:00, 29.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found directory /home/ray/Downloads/hwhw10/utility/data/raw/rock containing class rock\n",
            "Found directory /home/ray/Downloads/hwhw10/utility/data/raw/paper containing class paper\n",
            "Found directory /home/ray/Downloads/hwhw10/utility/data/raw/scissors containing class scissors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 29.34it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 16.51it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 30.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "data_eval = \"raw\"\n",
        "base_d = pathlib.Path(\"/home/ray/Downloads/hwhw10/utility/data\")\n",
        "dataset_test = create_user_testdata(base_d,data_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ade794bcabcd4a32b07e38944d4f5dd1"
          ]
        },
        "id": "bf_kY603EvW3",
        "outputId": "99adbba8-57e8-4fda-f197-fbf118efab07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results on individual raw inputs: \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ade794bcabcd4a32b07e38944d4f5dd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=0, continuous_update=False, description='i', max=14), Output()), _dom_cl‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<function __main__.resultsShow(i, data, model)>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "# Show results by processing a single validataion or testing image\n",
        "import matplotlib.pyplot as plt\n",
        "names = [\"rock\", \"paper\", \"scissors\"]\n",
        "\n",
        "%matplotlib inline\n",
        "def resultsShow(i, data, model):\n",
        "    guide = { 0:\"rock\",1:\"paper\",2:\"scissor\"}\n",
        "    d = data.iloc[i]\n",
        "    im = d[\"image\"]\n",
        "    l = d[\"label\"]\n",
        "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
        "    \n",
        "    imt = imr = skimage.transform.resize(im, (model.input_shape[1],model.input_shape[1]))\n",
        "    axs[0].imshow(im)\n",
        "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
        "    \n",
        "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
        "    axs[1].set_title(\"Network input\")\n",
        "    \n",
        "    outs = model.predict(np.array([imt]))\n",
        "    predicted = np.argmax(outs)\n",
        "    print(outs)\n",
        "    print(\"predicted label, %s\" % guide.get(predicted))\n",
        "    print(\"actual label, %s\"% guide.get(l))\n",
        "\n",
        "    axs[2].bar(np.array(range(len(names)))-0.5, outs[0,:], 1, color=\"gray\")\n",
        "    axs[2].set_ylim([0,1])\n",
        "    axs[2].set_xticks(range(len(names)))\n",
        "    axs[2].set_xticklabels(names)\n",
        "    axs[2].set_ylabel(\"probability\")\n",
        "    axs[2].set_xlabel(\"class\")\n",
        "    axs[2].set_title(\"Network output\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    #fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))    \n",
        "print(\"Results on individual {} inputs: \".format(dataset_test.loc[0].dn)) \n",
        "interact(resultsShow, i=widgets.IntSlider(min=0,max=len(dataset_test)-1, step=1, value=0, continuous_update=False), data=fixed(dataset_test.sample(len(dataset_test))), model=fixed(v2_raw))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OO04lXiEvW3"
      },
      "source": [
        "## 4. Model Comparison: original vs augmented\n",
        "- Did data augemntation help? \n",
        "- Which of the two NN versions benefited or suffered more from data augmentation? \n",
        "- Give an explanation/guestimate why this is the case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yBx0R5MEvW4",
        "outputId": "d986a607-e3ba-4137-9317-6356a9a86428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of pictures predicted correctly by model: 6\n",
            "Number of picutres in the dataset: 15\n",
            "Number of pictures predicted correctly by model: 8\n",
            "Number of picutres in the dataset: 15\n",
            "v1_aug on test: \n",
            "0.4\n",
            "v2_aug on test: \n",
            "0.5333333333333333\n"
          ]
        }
      ],
      "source": [
        "v1_t_aug = accuracy(X_test_example, y_test_example, v1_aug) \n",
        "v2_t_aug = accuracy(X_test_example, y_test_example, v2_aug) \n",
        "print(\"v1_aug on test: \")\n",
        "print(v1_t_aug)\n",
        "print(\"v2_aug on test: \")\n",
        "print(v2_t_aug)\n",
        "#v1_train_aug = accuracy(X_train, y_train, v1_aug) \n",
        "#v2_train_aug = accuracy(X_train, y_train, v2_aug) \n",
        "#print(\"v1_aug on train: \")\n",
        "#print(v1_train_aug)\n",
        "#print(\"v2_aug on train: \")\n",
        "#print(v2_train_aug)\n",
        "#v1_v_aug = accuracy(X_valid, y_valid, v1_aug) \n",
        "#v2_v_aug = accuracy(X_valid, y_valid, v2_aug) \n",
        "#print(\"v1_aug on validation: \")\n",
        "#print(v1_v_aug)\n",
        "#print(\"v2_aug on validation: \")\n",
        "#print(v2_v_aug)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "JWpSfFQ1EvW4"
      },
      "source": [
        "\n",
        "                        | training acc | validation acc | testing acc \n",
        "              cse217_v1 |     0.4643   |      0.4308    |    0.13\n",
        "    cse217_v1_augmented |     0.6556   |      0.6308    |    0.4\n",
        "      \n",
        "              cse217_v2 |     0.5740   |      0.4718    |    0.33\n",
        "    cse217_v2_augmented |     0.7985   |      0.7282     |    0.53\n",
        "\n",
        "\n",
        "In my opinion, the augmentation helped to increased the accuracy of our model, since most of the data output increased.\n",
        "v2 improved more from augmentation.\n",
        "they have different improvementsmay be because v1 model, v2 model take in different parameters counts into considerations, and after augmentation, the model can takes into more parameter into consideration, as they have different way of processing the information of data, which may be caused them to be benefited differently.\n",
        "but still, the augment's improvements are unclear. For example, it only increased 0.2 on testing dataset for v2 model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}